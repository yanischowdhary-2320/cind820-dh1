# -*- coding: utf-8 -*-
"""Cind820_Initial_Results.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cOiHxAGvuVO99-IcFSfcCjI9J5jbQ_Ax
"""

# Upload the csv file into google colab
from google.colab import files
uploaded = files.upload()

filename = list(uploaded.keys())[0]


# Then load the cleaned UCI income dataset into pandas
import io
import pandas as pd

# Converts file for pandas utilization
df = pd.read_csv(io.BytesIO(uploaded[filename]))

print("The Cleaned UCI Income Dataset")
print(df.shape)
print(df.head())

# Importing the libraries needed for Data Analysis and Modelling
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Machine learning tools needed for training dataset from scikit-learn
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE

# Initial Exploratory Data Analysis
print("\nSummary Statistics:")
print(df.describe(include='all'))

# Checking the target variable distribution (Income)
# Removing extra character that change the outcome
df['income'] = df['income'].astype(str).str.strip().str.replace('.','', regex = False)

# Mapping the text income levels for modeling
income_mapping = {'<=50K' : 0, '>50K' : 1}
df['income'] = df['income'].replace(income_mapping)

# Target Variable Distribution
print("\nTarget Variable Distribution:")
target_counts = df['income'].value_counts().rename(index={0 : '<=50K', 1 : '>50K'})
print(target_counts)

# Visual representation of the datasets Income Distribution
sns.countplot(data=df, x='income')
plt.title(" Income Distribution")
plt.xlabel("Income Category")
plt.ylabel("Count")
plt.xticks(ticks=[0, 1], labels=['<=50K', '>50K'])
plt.show

# A correlation heatmap for the numeric features in the dataset
plt.figure(figsize=(10,6))
sns.heatmap(df.select_dtypes(include= ['int64' , 'float64']).corr(), annot=True, cmap='coolwarm')
plt.title("Numeric Features Heatmap")
plt.show()

# Encoding Categorical variables
label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le
print("Encoded Categorical variables")

# The features and target
X = df.drop('income', axis = 1)
y = df['income']

# Splitting the Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

print("Training set:", X_train.shape)
print("test set:", X_test.shape)

# Handling class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)
print("Smote applied for class balancing")
print("The new class distribution", pd.Series(y_train_res).value_counts().rename(index={0 : '<=50K', 1: '>50K'}))

# Scaling the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_res)
X_test_scaled = scaler.transform(X_test)

print("Feature scaling has been completed")

# Using the Baseline models
# Decision tree and Logistic regression

models = {
    "Logistic Regression" : LogisticRegression(max_iter=1000),
    "Decision Tree" : DecisionTreeClassifier(random_state=42)
}

results = {}

for name, model in models.items():
    model.fit(X_train_scaled, y_train_res)
    y_pred = model.predict(X_test_scaled)

    results[name] = {
        "Accuracy" : accuracy_score(y_test, y_pred),
        "Precision" : precision_score(y_test, y_pred),
        "Recall" : recall_score(y_test, y_pred),
        "F1 Score" :f1_score(y_test, y_pred)
    }
    print(f"{name} Performance:")
    print(classification_report(y_test, y_pred, target_names=['<=50K', '>50K']))

# Using the Advanced Model
# Random Forest
    rf = RandomForestClassifier(n_estimators = 200, random_state=42)
    rf.fit(X_train_scaled, y_train_res)
    y_pred_rf = rf.predict(X_test_scaled)

    results["Random Forest"] = {
        "Accuracy" : accuracy_score(y_test, y_pred_rf),
        "Precision" : precision_score(y_test, y_pred_rf),
        "Recall" : recall_score(y_test, y_pred_rf),
        "F1 Score" : f1_score(y_test, y_pred_rf)
    }
    print("Random Forest Performance")
    print(classification_report(y_test, y_pred_rf, target_names=['<=50K', '>50K']))

    # Comparing the model results
results_df = pd.DataFrame(results).T
print("Model Comparison:")
print(results_df)

# Bar graph to show the accuracy results more clearly
sns.barplot(data=results_df.reset_index(), x='index' , y='Accuracy')
plt.title("Model Accuracy Comparison")
plt.ylabel("Accuracy")
plt.xlabel("Model")
plt.show

# The Feature importance for Random Forest
importances = rf.feature_importances_
feat_imp = pd.DataFrame({'Feature': X.columns, 'Importance' : importances}).sort_values(by='Importance', ascending = False)

plt.figure(figsize=(10,6))
sns.barplot(data=feat_imp.head(10), x='Importance' , y='Feature')
plt.title ("10 Most Important Features accoring to Random Forest")
plt.show()

print("Feature importance analysis completed")