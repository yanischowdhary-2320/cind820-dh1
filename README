Census Income Prediction Project

This project focuses on predicting whether an individual earns more than $50000 per year using the UCI census income
datset. The goal in the project is to clean, preprocess and prepare the dataset for machine learning models. 

The dataset contains demographic educational and occupational features that are relevant when it comes to income prediction
Understanding these patterns will help provide insight into other socioeconomic factors. 


Dataset : UCI Machine Learning Repository - Census Income (Adult) dataset
Source : https://archive.ics.uci.edu/dataset/20/census+income
Size : 48842 instances with 14 predictive features and 1 target variable. 
The features include a mix of both numeric and categorical features.
Numeric : age, education-num, capital-gain, capital-loss, hours-per-week
Categorical : workclass, education, marital-status, occupation, relationship, race, sex, native-country. 
Target Variable : Less than 50000 or More than 50000

Data Preparation

Data Cleaning: Replace or remove “?” entries to handle missing values.
Also stripped whitespace, and removed punctuations. (Kohavi, 1996)

Encoding: Apply One-Hot Encoding for categorical variables to avoid ordinal bias and retain interpretability. (Quinlan, 1996)

Scaling: Standardized the numerical features using StandardScaler to normalize features, which was crucial for gradient-based algorithms. (Kotsantis, 2007)

Balancing: Apply SMOTE (Synthetic Minority Oversampling Technique) to oversample the minority class (>50K). (Chawla, 2002)

Splitting: Divide dataset into training (70%) and testing (30%) subsets used stratified sampling to preserve the class proportions. (Friendman, 2001)


Exploratory Data Analysis : 
Visualized the income distribution to show that 76% made <= 50K and 24% made >50K
Identified correlations among the numeric features such as education level, age, and hours worked. 
I used heatmaps and count plots to highlight trends between the demographic variables and income.

The Model approach was use 
Logistic Regression which is Linear and able to interperate the baseline for binary classification

Decision Tree which his non-linear and captures interactions and relationships in the dataset

Random Forest which is an ensemble method and combines multiple trees for improved accuracy. 

Model Comparison:
                     Accuracy  Precision    Recall  F1 Score
Logistic Regression  0.761701   0.513102  0.751338  0.609777
Random Forest        0.840127   0.678861  0.673409  0.676124
Decision Tree        0.795460   0.579690  0.635039  0.606104

Using the Random forest ensemble method I was also able to find the 10 most influential predictors when it came to income
and they were as follows : 
marital-status = 0.17
age = 0.13
relationship = 0.12
fnlwgt = 0.11
hours-per-week = 0.09
education-num = 0.09
education = 0.08
capital-gain = 0.07
occupation = 0.06
workclass = 0.03

Tools used: 
Language	Python 3.10+
Libraries	Pandas, NumPy, Matplotlib, Seaborn
ML Frameworks	scikit-learn, imbalanced-learn
Environment	Google Colab
Data Source	UCI Machine Learning Repository: https://archive.ics.uci.edu/dataset/20/census+income

References 

Kohavi, R. (1996). Census Income [Dataset]. UCI Machine Learning Repository. https://doi.org/10.24432/C5GP7S

Kohavi, R. (1996). Scaling up the accuracy of naive-Bayes classifiers: a decision-tree hybrid. Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD), 202–207.


Dua, D., & Graff, C. (2019). UCI Machine Learning Repository: Census Income Dataset. University of California, Irvine. Retrieved from https://archive.ics.uci.edu/dataset/20/census+income


Kotsiantis, S. B. (2007). Supervised machine learning: A review of classification techniques. Informatica, 31(3), 249–268.


Quinlan, J. R. (1996). Improved use of continuous attributes in C4.5. Journal of Artificial Intelligence Research, 4, 77–90.


Friedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29(5), 1189–1232.


Chawla, N. V., Bowyer, K. W., Hall, L. O., & Kegelmeyer, W. P. (2002). SMOTE: Synthetic Minority Over-sampling Technique. Journal of Artificial Intelligence Research, 16, 321–357.



